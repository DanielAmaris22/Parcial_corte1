{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff567e2-0d04-4f6c-8066-091789dafebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7f22ac-b947-4d49-9c14-b29810468860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "from xgboost import XGBClassifier\n",
    "import re\n",
    "from pycaret.classification import setup, compare_models, create_model, tune_model, plot_model, evaluate_model, finalize_model, predict_model, save_model, load_model, get_config\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ec4ba1-8526-445f-aa10-8bf0275870a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/danie/Documents/Parcial_Python/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa6b25-ec7b-4bc7-b804-3f16ec1116e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ba_entr = pd.read_csv(path  + 'train.csv')\n",
    "ba_pru = pd.read_csv(path + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52549ff5-6e33-45ca-8e3e-add1bb51a504",
   "metadata": {},
   "outputs": [],
   "source": [
    "ba_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41227dcc-bec8-47d6-8d44-e18a1eac38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {'Graduate': 0, 'Dropout': 1, 'Enrolled': 2}\n",
    "ba_entr['Target'] = ba_entr['Target'].map(category_mapping)\n",
    "ba_entr['Target'] = ba_entr['Target'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22620b03-41de-4aa9-8d94-9bc8d91a2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ['Marital status', 'Application mode', 'Application order',\n",
    "       'Course', 'Daytime/evening attendance', 'Previous qualification',\n",
    "       'Nacionality', \"Mother's qualification\", \"Father's qualification\",\n",
    "       \"Mother's occupation\", \"Father's occupation\", 'Displaced', \n",
    "       'Educational special needs', 'Debtor','Tuition fees up to date', \n",
    "       'Gender', 'Scholarship holder', 'International']\n",
    "\n",
    "for k in ct:\n",
    "  ba_entr[k] = ba_entr[k].astype(\"O\")\n",
    "  ba_pru[k] = ba_pru[k].astype(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b2b34f-d13f-4fad-81da-95502891bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "formato = pd.DataFrame({'Variable': list(ba_entr.columns),'Formato': ba_entr.dtypes })\n",
    "ft = pd.merge(ft,formato,on=[\"Variable\"],how=\"left\")\n",
    "ft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427053dc-30c3-4f28-a033-2a24bdbadac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuantitativas = list(formato.loc[formato[\"Formato\"]!=\"object\",\"Variable\"])\n",
    "cuantitativas = [x for x in cuantitativas if x not in [\"id\",\"Target\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacf186-c1a8-4c14-9d6f-a0eda83ee38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in cuantitativas:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxplot(x=\"Target\",y=k,data=ba_entr)\n",
    "    plt.title(\"Boxplot de \"+k+\" vs Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78d04c-084a-4ff1-90ac-166ad9f160f0",
   "metadata": {},
   "source": [
    "Los diferentes boxplot muestran una comparación de las calificaciones previas entre tres grupos definidos por la variable Target (estado del estudiante: graduado, matriculado y retirado). Grade vs Target: A pesar de que existen algunos valores atípicos en cada grupo, en general, las calificaciones previas se distribuyen de manera similar entre los tres grupos. La mediana y el rango intercuartílico son bastante parecidos, lo que sugiere que no hay diferencias significativas en el rendimiento académico previo entre estos grupos.\n",
    "\n",
    "Admission grade vs Target: Se puede identificar que las calificaciones de admisión se distribuyen de manera similar entre los tres grupos del \"Target\", con una mediana y un rango intercuartílico similares. Sin embargo, existen algunos valores atípicos con puntuaciones muy altas o muy bajas en cada grupo. Hay casos individuales que se desvían significativamente de la media.\n",
    "\n",
    "Enrollement vs Target: Por otra parte en este boxplot se puede evidenciar que la edad de inscripción es más baja en el grupo 0 y ligeramente más alta en los grupos 1 y 2, siendo 1 el grupo con mayor rango intercuartílico. Sin embargo, existe una gran variabilidad en la edad de inscripción dentro de cada grupo, con algunos individuos inscribiéndose a edades significativamente mayores. Los valores atípicos (puntos individuales por encima y por debajo de las cajas) sugieren la presencia de casos excepcionales en cada grupo. Es decir, aunque existe una tendencia general de edades más altas en los grupos 1 y 2 comparado con el grupo 0, la variabilidad es considerable.\n",
    "\n",
    "1st sem credited vs Target: No hay una gran diferencia en el número de unidades curriculares acreditadas en el primer semestre entre los tres grupos del \"Target\".\n",
    "\n",
    "1st sem enrolled vs Target: A diferencia del anterior boxplot el número de unidades curriculares en las que se inscribieron los estudiantes es similar en los tres grupos del \"Target\", con una mediana y un rango intercuartílico similares. A diferencia del anterior los valores atípicos toman valores muy altos o muy bajos en cada grupo.\n",
    "\n",
    "1st evaluation vs Target: El boxplot indica que el número de unidades curriculares evaluadas no parece ser un factor determinante para distinguir entre los tres grupos del \"Target\", al menos basándonos en esta visualización.\n",
    "\n",
    "1st sem approved vs Target: Este boxplot nos deja evidenciar que se encuentran valores atípicos tanto altos como bajos a diferencia de la categoria 1 y 2 los cuales presentan una ligera similitud en el rango intercuartílico.\n",
    "\n",
    "1st sem grade vs Target: En os grupos 0 y 2 se evidencia una distribución similar, con una mediana de alrededor de 5 unidades curriculares aprobadas y varios valores atípicos, lo que indica que algunos estudiantes en estos grupos aprobaron significativamente más unidades que el promedio. En cambio, el grupo 1 tiene una mediana más baja, cerca de 3 unidades curriculares, y menos valores atípicos, con una dispersión más concentrada alrededor de la mediana. La dispersión es mayor en los grupos 0 y 2, reflejando una mayor variabilidad en las unidades aprobadas en comparación con el grupo 1.\n",
    "\n",
    "1 st sem without evaluation vs Target: Se observa una mayor similitud entre los tres grupos, con medianas cercanas y una dispersión más homogénea, representada por un rango intercuartílico estrecho y valores atípicos menos destacados. Esto sugiere una percepción diferente de la variabilidad y la similitud entre los grupos dependiendo de la interpretación.\n",
    "\n",
    "2nd sem credited vs Target: En este grafico podemos observar que en las tres categorias se encuentra una dispersión similar con valores atipicos hasta 17.5 unidades curriculares.\n",
    "\n",
    "2nd sem enrolled vs Target: En los tres grupos se presentan una distribución bastante similar, con la mayoría de los estudiantes matriculados entre 5 y 10 unidades curriculares. Las medianas de los grupos son cercanas, lo que indica que, en promedio, los estudiantes se matricularon en una cantidad similar de unidades. El rango intercuartílico también es parecido entre los grupos, lo que sugiere que los datos están concentrados en un rango pequeño. Se observan algunos valores atípicos en cada grupo, representando a aquellos estudiantes que se matricularon en un número significativamente mayor de unidades en comparación con sus compañeros.\n",
    "\n",
    "2nd sem evaluation vs Target:Se logra evidenciar que en la categoría 1 las unidades curriculares evaluadas estan entre 0 y 9 unidades curriculares a comparación de la categoría 0 y 2 las cuales tienen una dispersión similar de los datos, sin embargo en las tres categorias se encuentran valores atípicos que representan aquellos estudiantes evaluados.\n",
    "\n",
    "2nd sem grade vs Target: Los tres grupos presentan distribuciones bastante similares en cuanto a las calificaciones de unidades curriculares, con la mayoría de los estudiantes obteniendo calificaciones en un rango de 10 a 15 unidades. Aunque hay algunos valores atípicos que representan estudiantes con calificaciones significativamente mayores, el rango intercuartílico es comparable en los tres grupos, indicando que los datos están concentrados en un rango reducido. Las medianas de los grupos son muy cercanas, lo que sugiere que, en promedio, los estudiantes de los tres grupos obtienen calificaciones en una cantidad similar de unidades.\n",
    "\n",
    "2nd sem without evaluation vs Target: En el gráfico se observa que las tres categorías exhiben una dispersión similar, con algunos valores atípicos que se extienden hasta 12 unidades curriculares.\n",
    "\n",
    "Unemployement vs Target: Se puede evidenciar que los tres grupos tienen distribuciones similares de la tasa de desempleo, con la mayoría de los datos ubicados entre un 8% y un 16%. Las medianas son casi idénticas, indicando tasas promedio similares entre los grupos, y el rango intercuartílico es comparable, lo que sugiere una dispersión similar en torno a la mediana. No hay valores atípicos significativos, lo que refuerza la idea de que no hay diferencias notables en la tasa de desempleo entre los grupos.\n",
    "\n",
    "Inflation vs Target: El boxplot revela que los tres grupos tienen una distribución bastante similar en cuanto a la tasa de inflación, con la mayoría de los datos situados entre el 0% y el 3%. Las medianas de los grupos son muy cercanas, lo que indica tasas promedio similares, y el rango intercuartílico es comparable, sugiriendo una dispersión similar en torno a la mediana. No se observan valores atípicos significativos, lo que refuerza la idea de que no hay diferencias notables en la tasa de inflación entre los grupos.\n",
    "\n",
    "GDP vs Target: El ultimo boxplot indica que los tres grupos presentan una distribución comparable en términos de crecimiento del PIB, con la mayoría de los valores ubicados entre -2% y 2%. Las medianas son casi idénticas, lo que sugiere tasas promedio de crecimiento similares, y el rango intercuartílico muestra una dispersión uniforme alrededor de la mediana. La ausencia de valores atípicos significativos refuerza la idea de que no hay diferencias importantes en el crecimiento del PIB entre los grupos. En conjunto, el crecimiento del PIB se comporta de manera homogénea en los tres grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15477a0b-9ebd-4e3a-8a17-9b3e7d39b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asociacion(data):\n",
    "    categoricas=data.columns[data.dtypes=='O']\n",
    "    V1=np.array(categoricas); V2=np.array(categoricas)\n",
    "    grilla=np.meshgrid(V1,V2)\n",
    "    grilla=pd.DataFrame({'Var1':grilla[0].ravel(),'Var2':grilla[1].ravel()})\n",
    "    p_value=[stats.chi2_contingency(pd.DataFrame(pd.crosstab(data[grilla.iloc[x]['Var1']],data[grilla.iloc[x]['Var2']])))[1] for x in range(grilla.shape[0])]\n",
    "\n",
    "    grilla['p_value']=p_value\n",
    "    grilla2=grilla.pivot(index='Var1',columns='Var2',values='p_value')\n",
    "    plt.figure(figsize=(10,8))\n",
    "    gr=sns.heatmap(grilla2,linewidths=0.01,annot=True,fmt='.2f',cmap='summer')\n",
    "    gr.set_title('Grilla de p valores en prueba chi cuadrado para verificar asociación entre variables')\n",
    "    plt.xlabel(\"\"); plt.ylabel(\"\"); plt.yticks(rotation=0); plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    return grilla,grilla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0f650-aba3-4462-858f-1851fa46de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grilla_asociacion, grilla_asociacion2 = asociacion(ba_entr.drop([\"id\"],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53547aef-08be-4032-93a3-5022e3473f0e",
   "metadata": {},
   "source": [
    "Un p-valor cercano a 0 indica una evidencia de que las variables no están asociadas, es decir, las categorías de una variable son independientes de las categorías de la otra. En contraste, un p-valor cercano a 1 sugiere que las variables son dependientes, mostrando una fuerte evidencia de asociación. La gráfica se presenta en forma de una matriz de colores, donde los tonos amarillos indican p-valores altos mayor evidencia de asociación y los tonos verdes p-valores altos menor evidencia de asociación.\n",
    "\n",
    "Se puede evidenciar que la mayoria de las variables no tienen una asociación dado que encontramos en gran proporción ceros en la matriz, exceptuando una fuerte asociación entre variables como: Nacionalidad y ocupación del padre. Nacionalidad y ocupación de la madre. Nacionalidad y calificación previa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fe89bdc-9564-4c0b-ab17-edef95f5e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_FLOW_PARCIAL:\n",
    "    def __init__(self, met, mod):\n",
    "        self.met = met #selecciona si el modelo es con o sin ing. de variables\n",
    "        self.mod = mod #selecciona alguno de los tres mejores modelos\n",
    "        pass\n",
    "        \n",
    "    def load_data(self):\n",
    "        ba_entr = pd.read_csv(path  + 'train.csv')\n",
    "        ba_pru = pd.read_csv(path + \"test.csv\")\n",
    "        return ba_entr, ba_pru\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        ##TRATAMIENTO DE DATOS\n",
    "        category_mapping = {'Graduate': 0, 'Dropout': 1, 'Enrolled': 2}\n",
    "        \n",
    "        if self.met == True:\n",
    "            self.ba_entr['Target'] = self.ba_entr['Target'].map(category_mapping).astype('object')\n",
    "        else:\n",
    "            self.ba_entr['Target'] = self.ba_entr['Target'].map(category_mapping)\n",
    "            \n",
    "        ct = ['Marital status', 'Application mode', 'Application order',\n",
    "        'Course', 'Daytime/evening attendance', 'Previous qualification',\n",
    "        'Nacionality', \"Mother's qualification\", \"Father's qualification\",\n",
    "        \"Mother's occupation\", \"Father's occupation\", 'Displaced', \n",
    "        'Educational special needs', 'Debtor','Tuition fees up to date', \n",
    "        'Gender', 'Scholarship holder', 'International']\n",
    "        \n",
    "        for k in ct:\n",
    "            self.ba_entr[k] = self.ba_entr[k].astype(\"O\")\n",
    "            self.ba_pru[k] = self.ba_pru[k].astype(\"O\")\n",
    "        \n",
    "        return self.ba_entr, self.ba_pru\n",
    "            \n",
    "    def train_model(self):\n",
    "\n",
    "        formato = pd.DataFrame({'Variable': list(self.ba_entr.columns), 'Formato': self.ba_entr.dtypes })\n",
    "        \n",
    "        cuantitativas = list(formato.loc[formato[\"Formato\"]!=\"object\",\"Variable\"])\n",
    "        cuantitativas = [x for x in cuantitativas if x not in [\"id\",\"Target\"]]\n",
    "        \n",
    "        if self.met == True:\n",
    "        \n",
    "            ## Variables al cuadrado\n",
    "            base_cuadrado = self.ba_entr.get(cuantitativas).copy()\n",
    "            base_cuadrado[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "\n",
    "            var_names2, pvalue1 = [], []\n",
    "\n",
    "            for k in cuantitativas:\n",
    "                base_cuadrado[k+\"_2\"] = base_cuadrado[k] ** 2\n",
    "\n",
    "                # Prueba de Kruskal sin logaritmo\n",
    "                mue1 = base_cuadrado.loc[base_cuadrado[\"Target\"]==0,k+\"_2\"].to_numpy()\n",
    "                mue2 = base_cuadrado.loc[base_cuadrado[\"Target\"]==1,k+\"_2\"].to_numpy()\n",
    "                mue3 = base_cuadrado.loc[base_cuadrado[\"Target\"]==2,k+\"_2\"].to_numpy()\n",
    "\n",
    "                p1 = stats.kruskal(mue1,mue2,mue3)[1]\n",
    "\n",
    "                # Guardar p values y variables\n",
    "                var_names2.append(k+\"_2\")\n",
    "                pvalue1.append(np.round(p1,2))\n",
    "            pcuadrado1 = pd.DataFrame({'Variable2':var_names2,'p value':pvalue1})\n",
    "            pcuadrado1[\"criterio\"] = pcuadrado1.apply(lambda row: 1 if row[\"p value\"]<=0.10 else 0,axis = 1)\n",
    "\n",
    "            ## Interacciones cuantitativas\n",
    "            lista_inter = list(combinations(cuantitativas,2))\n",
    "            base_interacciones = self.ba_entr.get(cuantitativas).copy()\n",
    "            var_interaccion, pv1 = [], []\n",
    "            base_interacciones[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "\n",
    "            for k in lista_inter:\n",
    "                base_interacciones[k[0]+\"__\"+k[1]] = base_interacciones[k[0]] * base_interacciones[k[1]]\n",
    "\n",
    "                # Prueba de Kruskal\n",
    "                mue1 = base_interacciones.loc[base_interacciones[\"Target\"]==0,k[0]+\"__\"+k[1]].to_numpy()\n",
    "                mue2 = base_interacciones.loc[base_interacciones[\"Target\"]==1,k[0]+\"__\"+k[1]].to_numpy()\n",
    "                mue3 = base_interacciones.loc[base_interacciones[\"Target\"]==2,k[0]+\"__\"+k[1]].to_numpy()\n",
    "                p1 = stats.kruskal(mue1,mue2,mue3)[1]\n",
    "\n",
    "                var_interaccion.append(k[0]+\"__\"+k[1])\n",
    "                pv1.append(np.round(p1,2))\n",
    "            pxy = pd.DataFrame({'Variable':var_interaccion,'p value':pv1})\n",
    "            pxy[\"criterio\"] = pxy.apply(lambda row: 1 if row[\"p value\"]<=0.10 else 0, axis = 1)\n",
    "            \n",
    "            ## Razones\n",
    "            raz1 = [(x,y) for x in cuantitativas for y in cuantitativas]\n",
    "            base_razones1 = self.ba_entr.get(cuantitativas).copy()\n",
    "            base_razones1[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "\n",
    "            var_nm, pval = [], []\n",
    "            for j in raz1:\n",
    "                if j[0]!=j[1]:\n",
    "                    base_razones1[j[0]+\"__coc__\"+j[1]] = base_razones1[j[0]] / (base_razones1[j[1]]+0.01)\n",
    "\n",
    "                    # Prueba de Kruskal\n",
    "                    mue1 = base_razones1.loc[base_razones1[\"Target\"]==0,j[0]+\"__coc__\"+j[1]].to_numpy()\n",
    "                    mue2 = base_razones1.loc[base_razones1[\"Target\"]==1,j[0]+\"__coc__\"+j[1]].to_numpy()\n",
    "                    mue3 = base_razones1.loc[base_razones1[\"Target\"]==2,j[0]+\"__coc__\"+j[1]].to_numpy()\n",
    "                    p1 = stats.kruskal(mue1,mue2,mue3)[1]\n",
    "        \n",
    "                    # Guardar valores\n",
    "                    var_nm.append(j[0]+\"__coc__\"+j[1])\n",
    "                    pval.append(np.round(p1,2))\n",
    "            prazones = pd.DataFrame({'Variable':var_nm,'p value':pval})\n",
    "            prazones[\"criterio\"] = prazones.apply(lambda row: 1 if row[\"p value\"]<=0.10 else 0, axis = 1)\n",
    "            \n",
    "            ## Interacciones categóricas\n",
    "            categoricas = list(formato.loc[formato[\"Formato\"]==\"O\",\"Variable\"])\n",
    "            categoricas = [x for x in categoricas if x not in [\"id\",\"Target\"]]\n",
    "\n",
    "            def nombre_(x):\n",
    "              return \"C\"+str(x)\n",
    "            cb = list(combinations(categoricas,2))\n",
    "            p_value, modalidades, nombre_var = [], [], []\n",
    "\n",
    "            base2 = self.ba_entr.get(categoricas).copy()\n",
    "            for k in base2.columns:\n",
    "              base2[k] = base2[k].map(nombre_)\n",
    "\n",
    "            base2[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "\n",
    "            for k in range(len(cb)):\n",
    "                # Variable con interacción\n",
    "                base2[cb[k][0]] = base2[cb[k][0]]\n",
    "                base2[cb[k][1]] = base2[cb[k][1]]\n",
    "\n",
    "                base2[cb[k][0]+\"__\"+cb[k][1]] = base2[cb[k][0]] + \"__\" + base2[cb[k][1]]\n",
    "\n",
    "                # Prueba chi cuadrado\n",
    "                c1 = pd.DataFrame(pd.crosstab(base2[\"Target\"],base2[cb[k][0]+\"__\"+cb[k][1]]))\n",
    "                pv = stats.chi2_contingency(c1)[1]\n",
    "\n",
    "                # Número de modalidades por categoría\n",
    "                mod_ = len(base2[cb[k][0]+\"__\"+cb[k][1]].unique())\n",
    "\n",
    "                # Guardar p value y modalidades\n",
    "                nombre_var.append(cb[k][0]+\"__\"+cb[k][1])\n",
    "                modalidades.append(mod_)\n",
    "                p_value.append(pv)\n",
    "            pc = pd.DataFrame({'Variable':nombre_var,'Num Modalidades':modalidades,'p value':p_value})\n",
    "            pc.loc[(pc[\"p value\"]<=0.20) & (pc[\"Num Modalidades\"]<=8),].sort_values([\"p value\"],ascending=True)\n",
    "            ## Dummies categóricas más significativas (p value <= 0.20 y bajo número de modalidades)\n",
    "            def indicadora(x):\n",
    "              if x==True:\n",
    "                return 1\n",
    "              else:\n",
    "                return 0\n",
    "\n",
    "            seleccion1 = list(pc.loc[(pc[\"p value\"]<=0.20) & (pc[\"Num Modalidades\"]<=8),\"Variable\"])\n",
    "            sel1 = base2.get(seleccion1)\n",
    "\n",
    "\n",
    "            ## convertir categorica a numerica: dummies\n",
    "            contador = 0\n",
    "            lb1 = pd.DataFrame()\n",
    "            for k in sel1:\n",
    "                if contador==0:\n",
    "                    lb1 = pd.get_dummies(sel1[k],drop_first=True)\n",
    "                    lb1.columns = [k + \"_\" + x for x in lb1.columns]\n",
    "                else:\n",
    "                    lb2 = pd.get_dummies(sel1[k],drop_first=True)\n",
    "                    lb2.columns = [k + \"_\" + x for x in lb2.columns]\n",
    "                    lb1 = pd.concat([lb1,lb2],axis=1)\n",
    "                    lb1\n",
    "                contador = contador + 1\n",
    "            \n",
    "            for k in lb1.columns:\n",
    "              lb1[k] = lb1[k].map(indicadora)\n",
    "\n",
    "            lb1[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "\n",
    "            ## Interacción cuantitativa vs categórica\n",
    "            cat_cuanti = [(x,y) for x in cuantitativas for y in categoricas]\n",
    "            v1, v2, pvalores_min, pvalores_max  = [], [], [], []\n",
    "\n",
    "            for j in cat_cuanti:\n",
    "                k1 = j[0]\n",
    "                k2 = j[1]\n",
    "\n",
    "                g1 = pd.get_dummies(self.ba_entr[k2])\n",
    "                lt1 = list(g1.columns)\n",
    "\n",
    "                for k in lt1:\n",
    "                    g1[k] = g1[k] * self.ba_entr[k1]\n",
    "\n",
    "                g1[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "                pvalues_c = []\n",
    "                for y in lt1:\n",
    "                    mue1 = g1.loc[g1[\"Target\"]==0,y].to_numpy()\n",
    "                    mue2 = g1.loc[g1[\"Target\"]==1,y].to_numpy()\n",
    "                    mue3 = g1.loc[g1[\"Target\"]==2,y].to_numpy()\n",
    "        \n",
    "                    try:\n",
    "                      pval = (stats.kruskal(mue1,mue2,mue3)[1]<=0.20)\n",
    "\n",
    "                      if pval==True:\n",
    "                          pval = 1\n",
    "                      else:\n",
    "                          pval = 0\n",
    "                    except ValueError:\n",
    "                      pval = 0\n",
    "                    pvalues_c.append(pval)\n",
    "\n",
    "                min_ = np.min(pvalues_c) # Se revisa si alguna de las categorías no es significativa\n",
    "                max_ = np.max(pvalues_c) # Se revisa si alguna de las categorías es significativa\n",
    "                v1.append(k1) # nombre de la variable 1\n",
    "                v2.append(k2) # nombre de la variable 2\n",
    "                pvalores_min.append(np.round(min_,2))\n",
    "                pvalores_max.append(np.round(max_,2))\n",
    "            pc2 = pd.DataFrame({'Cuantitativa':v1,'Categórica':v2,'p value':pvalores_min, 'p value max':pvalores_max})\n",
    "            pc2.loc[(pc2[\"p value\"]==1) & (pc2[\"p value max\"]==1),]\n",
    "\n",
    "            ## Base de Feature Enginnering\n",
    "            v1 = list(pc2.loc[(pc2[\"p value\"]==1) & (pc2[\"p value max\"]==1),\"Cuantitativa\"])\n",
    "            v2 = list(pc2.loc[(pc2[\"p value\"]==1) & (pc2[\"p value max\"]==1),\"Categórica\"])\n",
    "\n",
    "            for j in range(len(v1)):\n",
    "\n",
    "                if j==0:\n",
    "                    g1 = pd.get_dummies(self.ba_entr[v2[j]],drop_first=True)\n",
    "                    lt1 = list(g1.columns)\n",
    "                    for k in lt1:\n",
    "                        g1[k] = g1[k] * self.ba_entr[v1[j]]\n",
    "                    g1.columns = [v1[j] + \"_\" + v2[j] + \"_\" + str(x) for x in lt1]\n",
    "                else:\n",
    "                    g2 = pd.get_dummies(self.ba_entr[v2[j]],drop_first=True)\n",
    "                    lt1 = list(g2.columns)\n",
    "                    for k in lt1:\n",
    "                        g2[k] = g2[k] * self.ba_entr[v1[j]]\n",
    "                    g2.columns = [v1[j] + \"_\" + v2[j] + \"_\" + str(x) for x in lt1]\n",
    "                    g1 = pd.concat([g1,g2],axis=1)\n",
    "\n",
    "            g1[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "            #### SELECCION DE VARIABLES CON XGBOOST\n",
    "            var_cuad = list(pcuadrado1[\"Variable2\"])\n",
    "            base_modelo1 = base_cuadrado.get(var_cuad+[\"Target\"])\n",
    "            base_modelo1[\"Target\"] = base_modelo1[\"Target\"].map(int) ## convertir en enteros las categorias\n",
    "            cov = list(base_modelo1.columns)\n",
    "            cov = [x for x in cov if x not in [\"Target\"]]\n",
    "\n",
    "            X1 = base_modelo1.get(cov)\n",
    "            y1 = base_modelo1.get([\"Target\"])\n",
    "\n",
    "            modelo1 = XGBClassifier()\n",
    "            modelo1 = modelo1.fit(X1,y1)\n",
    "\n",
    "            importancias = modelo1.feature_importances_\n",
    "            imp1 = pd.DataFrame({'Variable':X1.columns,'Importancia':importancias})\n",
    "            imp1[\"Importancia\"] = imp1[\"Importancia\"] * 100 / np.sum(imp1[\"Importancia\"])\n",
    "            imp1 = imp1.sort_values([\"Importancia\"],ascending=False)\n",
    "            imp1.index = range(imp1.shape[0])\n",
    "\n",
    "            var_int = list(pxy[\"Variable\"])\n",
    "            base_modelo2 = base_interacciones.get(var_int+[\"Target\"])\n",
    "            base_modelo2[\"Target\"] = base_modelo2[\"Target\"].map(int)\n",
    "            cov = list(base_modelo2.columns)\n",
    "            cov = [x for x in cov if x not in [\"Target\"]]\n",
    "\n",
    "            X2 = base_modelo2.get(cov)\n",
    "            y2 = base_modelo2.get([\"Target\"])\n",
    "\n",
    "            modelo2 = XGBClassifier()\n",
    "            modelo2 = modelo2.fit(X2,y2)\n",
    "\n",
    "            importancias = modelo2.feature_importances_\n",
    "            imp2 = pd.DataFrame({'Variable':X2.columns,'Importancia':importancias})\n",
    "            imp2[\"Importancia\"] = imp2[\"Importancia\"] * 100 / np.sum(imp2[\"Importancia\"])\n",
    "            imp2 = imp2.sort_values([\"Importancia\"],ascending=False)\n",
    "            imp2.index = range(imp2.shape[0])\n",
    "\n",
    "            var_raz = list(prazones[\"Variable\"])\n",
    "            base_modelo3 = base_razones1.get(var_raz+[\"Target\"])\n",
    "            base_modelo3[\"Target\"] = base_modelo3[\"Target\"].map(int)\n",
    "            cov = list(base_modelo3.columns)\n",
    "            cov = [x for x in cov if x not in [\"Target\"]]\n",
    "\n",
    "            X3 = base_modelo3.get(cov)\n",
    "            y3 = base_modelo3.get([\"Target\"])\n",
    "\n",
    "            modelo3 = XGBClassifier()\n",
    "            modelo3 = modelo3.fit(X3,y3)\n",
    "\n",
    "            importancias = modelo3.feature_importances_\n",
    "            imp3 = pd.DataFrame({'Variable':X3.columns,'Importancia':importancias})\n",
    "            imp3[\"Importancia\"] = imp3[\"Importancia\"] * 100 / np.sum(imp3[\"Importancia\"])\n",
    "            imp3 = imp3.sort_values([\"Importancia\"],ascending=False)\n",
    "            imp3.index = range(imp3.shape[0])\n",
    "\n",
    "            lb1[\"Target\"] = lb1[\"Target\"].map(int)\n",
    "            cov = list(lb1.columns)\n",
    "            cov = [x for x in cov if x not in [\"Target\"]]\n",
    "\n",
    "            X4 = lb1.get(cov)\n",
    "            y4 = lb1.get([\"Target\"])\n",
    "\n",
    "            modelo4 = XGBClassifier()\n",
    "            modelo4 = modelo4.fit(X4,y4)\n",
    "\n",
    "            importancias = modelo4.feature_importances_\n",
    "            imp4 = pd.DataFrame({'Variable':X4.columns,'Importancia':importancias})\n",
    "            imp4[\"Importancia\"] = imp4[\"Importancia\"] * 100 / np.sum(imp4[\"Importancia\"])\n",
    "            imp4 = imp4.sort_values([\"Importancia\"],ascending=False)\n",
    "            imp4.index = range(imp4.shape[0])\n",
    "\n",
    "            g1[\"Target\"] = g1[\"Target\"].map(int)\n",
    "            cov = list(g1.columns)\n",
    "            cov = [x for x in cov if x not in [\"Target\"]]\n",
    "\n",
    "            X5 = g1.get(cov)\n",
    "            y5 = g1.get([\"Target\"])\n",
    "\n",
    "            modelo5 = XGBClassifier()\n",
    "            modelo5 = modelo5.fit(X5,y5)\n",
    "\n",
    "            importancias = modelo5.feature_importances_\n",
    "            imp5 = pd.DataFrame({'Variable':X5.columns,'Importancia':importancias})\n",
    "            imp5[\"Importancia\"] = imp5[\"Importancia\"] * 100 / np.sum(imp5[\"Importancia\"])\n",
    "            imp5 = imp5.sort_values([\"Importancia\"],ascending=False)\n",
    "            imp5.index = range(imp5.shape[0])\n",
    "\n",
    "            #### VARIABLES MAS IMPORTANTES POR XGBOOST EN CADA CASO ####\n",
    "            c2 = list(imp1.iloc[0:3,0]) # Variables al cuadrado\n",
    "            cxy = list(imp2.iloc[0:3,0]) # Interacciones cuantitativas\n",
    "            razxy = list(imp3.iloc[0:3,0]) # Razones\n",
    "            catxy = list(imp4.iloc[0:3,0]) # Interacciones categóricas\n",
    "            cuactxy = list(imp5.iloc[0:3,0]) # Interacción cuantitativa y categórica\n",
    "\n",
    "            # Variables cuantitativas (Activar D1)\n",
    "            D1 = self.ba_entr.get(cuantitativas).copy()\n",
    "\n",
    "            # Variables categóricas\n",
    "            D2 = self.ba_entr.get(categoricas).copy()\n",
    "            for k in categoricas:\n",
    "              D2[k] = D2[k].map(nombre_)\n",
    "            D4 = D2.copy()\n",
    "\n",
    "            # Variables al cuadrado (Activar D1)\n",
    "            cuadrado = [re.findall(r'(.+)_\\d+', item) for item in c2]\n",
    "            cuadrado = [x[0] for x in cuadrado]\n",
    "\n",
    "            for k in cuadrado:\n",
    "              D1[k+\"_2\"] = D1[k] ** 2\n",
    "            \n",
    "            # Interacciones cuantitativas (Activar D1)\n",
    "            result = [re.findall(r'([A-Za-z\\s\\(\\)0-9]+)', item) for item in cxy]\n",
    "\n",
    "            for k in result:\n",
    "              D1[k[0]+\"__\"+k[1]] = D1[k[0]] * D1[k[1]]\n",
    "\n",
    "            # Razones\n",
    "            result2 = [re.findall(r'(.+)__coc__(.+)', item) for item in razxy]\n",
    "            for k in result2:\n",
    "              k2 = k[0]\n",
    "              D1[k2[0]+\"__coc__\"+k2[1]] = D1[k2[0]] / (D1[k2[1]]+0.01)\n",
    "\n",
    "            # Interacciones categóricas\n",
    "            result3 = [re.search(r'([^_]+__[^_]+)', item).group(1).split('__') for item in catxy]\n",
    "            for k in result3:\n",
    "              D4[k[0]+\"__\"+k[1]] = D4[k[0]] + \"_\" + D4[k[1]]\n",
    "\n",
    "            # Interacción cuantitativa vs categórica\n",
    "            D5 = self.ba_entr.copy()\n",
    "            result4 = [re.search(r'(.+?)_(.+?)_1', item).groups() for item in cuactxy]\n",
    "            contador = 0\n",
    "            for k in result4:\n",
    "              col1, col2 = k[1], k[0] # categórica, cuantitativa\n",
    "              if contador == 0:\n",
    "                D51 = pd.get_dummies(D5[col1],drop_first=True)\n",
    "                for j in D51.columns:\n",
    "                  D51[j] = D51[j] * D5[col2]\n",
    "                D51.columns = [col2+\"_\"+col1+\"_\"+ str(x) for x in D51.columns]\n",
    "              else:\n",
    "                D52 = pd.get_dummies(D5[col1],drop_first=True)\n",
    "                for j in D52.columns:\n",
    "                  D52[j] = D52[j] * D5[col2]\n",
    "                D52.columns = [col2+\"_\"+col1+\"_\"+ str(x) for x in D52.columns]\n",
    "                D51 = pd.concat([D51,D52],axis=1)\n",
    "              contador = contador + 1\n",
    "            #### BASE MODELO ####\n",
    "            B1 = pd.concat([D1,D4],axis=1)\n",
    "            base_modelo = pd.concat([B1,D51],axis=1)\n",
    "            base_modelo[\"Target\"] = self.ba_entr[\"Target\"].copy()\n",
    "            base_modelo[\"Target\"] = base_modelo[\"Target\"].map(int)\n",
    "\n",
    "            #### AUTOML ####\n",
    "            formatos = pd.DataFrame(base_modelo.dtypes).reset_index()\n",
    "            formatos.columns = [\"Variable\",\"Formato\"]\n",
    "            cuantitativas_bm = list(formatos.loc[formatos[\"Formato\"]!=\"object\",][\"Variable\"])\n",
    "            categoricas_bm = list(formatos.loc[formatos[\"Formato\"]==\"object\",][\"Variable\"])\n",
    "            cuantitativas_bm = [x for x in cuantitativas_bm if x not in [\"Target\"]]\n",
    "            categoricas_bm = [x for x in categoricas_bm if x not in [\"Target\"]]\n",
    "\n",
    "            # Configuración del experimento\n",
    "            exp_clf101 = setup(data=base_modelo,\n",
    "            target='Target',\n",
    "            session_id=123,\n",
    "            train_size=0.7,\n",
    "            numeric_features = cuantitativas_bm,\n",
    "            categorical_features = categoricas_bm,\n",
    "            fix_imbalance=False)\n",
    "        else:\n",
    "            formatos = pd.DataFrame(self.ba_entr.dtypes).reset_index()\n",
    "            formatos.columns = [\"Variable\",\"Formato\"]\n",
    "            cuantitativas_bm = list(formatos.loc[formatos[\"Formato\"]!=\"object\",][\"Variable\"])\n",
    "            categoricas_bm = list(formatos.loc[formatos[\"Formato\"]==\"object\",][\"Variable\"])\n",
    "            cuantitativas_bm = [x for x in cuantitativas_bm if x not in [\"Target\"]]\n",
    "            categoricas_bm = [x for x in categoricas_bm if x not in [\"Target\"]]\n",
    "\n",
    "            # Configuración del experimento\n",
    "            exp_clf101 = setup(data=self.ba_entr,\n",
    "            target='Target',\n",
    "            session_id=123,\n",
    "            train_size=0.7,\n",
    "            numeric_features = cuantitativas_bm,\n",
    "            categorical_features = categoricas_bm,\n",
    "            fix_imbalance=False)\n",
    "        \n",
    "        # Comparación de modelos\n",
    "        best_model = compare_models(sort='AUC') ##accuracy\n",
    "        return best_model, exp_clf101, cuantitativas, categoricas, c2, cxy, razxy, catxy, cuactxy\n",
    "        \n",
    "    def select_model(self, exp_clf101):\n",
    "        if self.mod == 1:\n",
    "            dt = create_model('lightgbm')\n",
    "            \n",
    "            import pickle\n",
    "            with open(path + 'best_model.pkl', 'wb') as model_file:\n",
    "                pickle.dump(dt, model_file)\n",
    "                \n",
    "            #### CONTINUAR OPTIMIZACIÓN ####\n",
    "            # Define the parameter grid for Grid Search\n",
    "            param_grid_bayesian = {\n",
    "                'n_estimators': [50,100,200],\n",
    "                'max_depth': [3,5,7],\n",
    "                'min_child_samples': [50,150,200]\n",
    "            }\n",
    "            # Perform Bayesian Search\n",
    "            tuned_dt = tune_model(dt, custom_grid=param_grid_bayesian, search_library='scikit-optimize', search_algorithm='bayesian',fold=5)\n",
    "        elif self.mod == 2:\n",
    "            dt2 = create_model('rf')  # random forest classifier\n",
    "            \n",
    "            import pickle\n",
    "            with open(path + 'best_model_2.pkl', 'wb') as model_file:\n",
    "                pickle.dump(dt2, model_file)\n",
    "                    \n",
    "            #### CONTINUAR OPTIMIZACION ####\n",
    "            # Define the parameter grid for Grid Search\n",
    "            param_grid_bayesian_rf = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "            }\n",
    "            # Perform Bayesian Search\n",
    "            tuned_dt = tune_model(dt2, custom_grid=param_grid_bayesian_rf, search_library='scikit-optimize', search_algorithm='bayesian',fold=5)\n",
    "        else:\n",
    "            dt3 = create_model('et')  # extra trees classifier\n",
    "            \n",
    "            import pickle\n",
    "            with open(path + 'best_model_3.pkl', 'wb') as model_file:\n",
    "                pickle.dump(dt3, model_file)\n",
    "                    \n",
    "            #### CONTINUAR OPTIMIZACIÓN ####\n",
    "            # Define the parameter grid for Grid Search\n",
    "            param_grid_bayesian_et = {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "            # Perform Bayesian Search\n",
    "            tuned_dt = tune_model(dt3, custom_grid=param_grid_bayesian_et, search_library='scikit-optimize', search_algorithm='bayesian',fold=5)\n",
    "        # Visualización exhaustiva del modelo\n",
    "        evaluate_model(tuned_dt)\n",
    "                \n",
    "        # Evaluar el modelo en el conjunto de prueba\n",
    "        predictions_test = predict_model(tuned_dt)\n",
    "    \n",
    "        predictions_train = predict_model(tuned_dt, data=exp_clf101.get_config('X_train'))\n",
    "    \n",
    "        y_train = get_config('y_train')\n",
    "        y_test = get_config('y_test')\n",
    "    \n",
    "        from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "        # Error de entrenamiento\n",
    "        u1 = accuracy_score(y_train,predictions_train[\"prediction_label\"])\n",
    "        # Error de test\n",
    "        u2 = accuracy_score(y_test,predictions_test[\"prediction_label\"])\n",
    "    \n",
    "        # Finalizar el modelo\n",
    "        final_dt = finalize_model(tuned_dt)\n",
    "        return final_dt, u1, u2\n",
    "        \n",
    "    def predict(self, final_dt, cuantitativas, categoricas, c2, cxy, razxy, catxy, cuactxy):\n",
    "        if self.met == True:\n",
    "            #### PREDICCIÓN NUEVOS DATOS ####\n",
    "            # Variables cuantitativas (Activar D1)\n",
    "            D1 = self.ba_pru.get(cuantitativas).copy()\n",
    "\n",
    "            # Variables categóricas\n",
    "            def nombre_(x):\n",
    "              return \"C\"+str(x)\n",
    "            \n",
    "            D2 = self.ba_pru.get(categoricas).copy()\n",
    "            for k in categoricas:\n",
    "              D2[k] = D2[k].map(nombre_)\n",
    "            D4 = D2.copy()\n",
    "\n",
    "            \n",
    "            # Variables al cuadrado (Activar D1)\n",
    "            cuadrado = [re.findall(r'(.+)_\\d+', item) for item in c2]\n",
    "            cuadrado = [x[0] for x in cuadrado]\n",
    "            for k in cuadrado:\n",
    "              D1[k+\"_2\"] = D1[k] ** 2\n",
    "\n",
    "            # Interacciones cuantitativas (Activar D1)\n",
    "            result = [re.findall(r'([A-Za-z\\s\\(\\)0-9]+)', item) for item in cxy]\n",
    "            for k in result:\n",
    "              D1[k[0]+\"__\"+k[1]] = D1[k[0]] * D1[k[1]]\n",
    "\n",
    "            # Razones\n",
    "            result2 = [re.findall(r'(.+)__coc__(.+)', item) for item in razxy]\n",
    "            for k in result2:\n",
    "              k2 = k[0]\n",
    "              D1[k2[0]+\"__coc__\"+k2[1]] = D1[k2[0]] / (D1[k2[1]]+0.01)\n",
    "\n",
    "            # Interacciones categóricas\n",
    "            result3 = [re.search(r'([^_]+__[^_]+)', item).group(1).split('__') for item in catxy]\n",
    "            for k in result3:\n",
    "              D4[k[0]+\"__\"+k[1]] = D4[k[0]] + \"_\" + D4[k[1]]\n",
    "\n",
    "            # Interacción cuantitativa vs categórica\n",
    "            D5 = self.ba_pru.copy()\n",
    "            result4 = [re.search(r'(.+?)_(.+?)_1', item).groups() for item in cuactxy]\n",
    "            contador = 0\n",
    "            for k in result4:\n",
    "              col1, col2 = k[1], k[0] # categórica, cuantitativa\n",
    "              if contador == 0:\n",
    "                D51 = pd.get_dummies(D5[col1],drop_first=True)\n",
    "                for j in D51.columns:\n",
    "                  D51[j] = D51[j] * D5[col2]\n",
    "                D51.columns = [col2+\"_\"+col1+\"_\"+ str(x) for x in D51.columns]\n",
    "              else:\n",
    "                D52 = pd.get_dummies(D5[col1],drop_first=True)\n",
    "                for j in D52.columns:\n",
    "                  D52[j] = D52[j] * D5[col2]\n",
    "                D52.columns = [col2+\"_\"+col1+\"_\"+ str(x) for x in D52.columns]\n",
    "                D51 = pd.concat([D51,D52],axis=1)\n",
    "              contador = contador + 1\n",
    "\n",
    "            B1 = pd.concat([D1,D4],axis=1)\n",
    "            base_modelo2 = pd.concat([B1,D51],axis=1)\n",
    "\n",
    "            df_test = base_modelo2.copy()\n",
    "        else:\n",
    "            df_test = self.ba_pru.copy()\n",
    "        # Realizar predicciones\n",
    "        predictions = predict_model(final_dt, data=df_test)\n",
    "\n",
    "        predictions[\"sc\"] = predictions.apply(lambda row: 1 if row[\"prediction_score\"]<0.9 else 0, axis = 1)\n",
    "\n",
    "        inverse_category_mapping = {0: 'Graduate', 1: 'Dropout', 2: 'Enrolled'}\n",
    "        predictions['prediction_label'] = predictions['prediction_label'].map(inverse_category_mapping)\n",
    "        predictions['prediction_label'] = predictions['prediction_label'].astype('category')\n",
    "        \n",
    "        #### ARCHIVO KAGGLE ####\n",
    "        # Create a DataFrame with 'id' and 'Exited' probabilities\n",
    "        result = pd.DataFrame({\n",
    "            'id': self.ba_pru[\"id\"],\n",
    "            'Target': predictions['prediction_label']\n",
    "        })\n",
    "\n",
    "        # Save the result to a CSV file\n",
    "        result.to_csv(path + 'modelopp.csv', index=False,sep=\",\")\n",
    "        return result\n",
    "    #def evaluate_model(self, u1, u2):\n",
    "        #return 100 * u2\n",
    "    def ML_FLOW(self):\n",
    "        try:\n",
    "            # Paso 1: Cargar datos\n",
    "            self.ba_entr, self.ba_pru = self.load_data()\n",
    "        \n",
    "            # Paso 2: Preprocesamiento\n",
    "            self.ba_entr, self.ba_pru = self.preprocessing()\n",
    "        \n",
    "            # Paso 3: Entrenamiento del modelo\n",
    "            best_model, exp_clf101, cuantitativas, categoricas, c2, cxy, razxy, catxy, cuactxy = self.train_model()\n",
    "         \n",
    "            # Paso 4: Selección del modelo\n",
    "            final_dt, u1, u2 = self.select_model(exp_clf101)\n",
    "        \n",
    "            # Paso 5: Predicción\n",
    "            result = self.predict(final_dt, cuantitativas, categoricas, c2, cxy, razxy, catxy, cuactxy)\n",
    "        \n",
    "            # Paso 6: Evaluación\n",
    "            metric = self.evaluate_model(u1, u2)\n",
    "            return {'success':True,'accuracy':metric}\n",
    "        except Exception as e:\n",
    "            return {'success':False,'message':str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f35d6c54-8639-4c69-8744-72974cda3c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0d460_row9_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0d460\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0d460_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_0d460_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0d460_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_0d460_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0d460_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_0d460_row1_col1\" class=\"data row1 col1\" >Target</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0d460_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_0d460_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0d460_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_0d460_row3_col1\" class=\"data row3 col1\" >(76518, 52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0d460_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_0d460_row4_col1\" class=\"data row4 col1\" >(76518, 145)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0d460_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_0d460_row5_col1\" class=\"data row5 col1\" >(53562, 145)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0d460_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_0d460_row6_col1\" class=\"data row6 col1\" >(22956, 145)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0d460_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_0d460_row7_col1\" class=\"data row7 col1\" >30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0d460_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
       "      <td id=\"T_0d460_row8_col1\" class=\"data row8 col1\" >21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0d460_row9_col0\" class=\"data row9 col0\" >Preprocess</td>\n",
       "      <td id=\"T_0d460_row9_col1\" class=\"data row9 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_0d460_row10_col0\" class=\"data row10 col0\" >Imputation type</td>\n",
       "      <td id=\"T_0d460_row10_col1\" class=\"data row10 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_0d460_row11_col0\" class=\"data row11 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_0d460_row11_col1\" class=\"data row11 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_0d460_row12_col0\" class=\"data row12 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_0d460_row12_col1\" class=\"data row12 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_0d460_row13_col0\" class=\"data row13 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_0d460_row13_col1\" class=\"data row13 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_0d460_row14_col0\" class=\"data row14 col0\" >Encoding method</td>\n",
       "      <td id=\"T_0d460_row14_col1\" class=\"data row14 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_0d460_row15_col0\" class=\"data row15 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_0d460_row15_col1\" class=\"data row15 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_0d460_row16_col0\" class=\"data row16 col0\" >Fold Number</td>\n",
       "      <td id=\"T_0d460_row16_col1\" class=\"data row16 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_0d460_row17_col0\" class=\"data row17 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_0d460_row17_col1\" class=\"data row17 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_0d460_row18_col0\" class=\"data row18 col0\" >Use GPU</td>\n",
       "      <td id=\"T_0d460_row18_col1\" class=\"data row18 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_0d460_row19_col0\" class=\"data row19 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_0d460_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_0d460_row20_col0\" class=\"data row20 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_0d460_row20_col1\" class=\"data row20 col1\" >clf-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0d460_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_0d460_row21_col0\" class=\"data row21 col0\" >USI</td>\n",
       "      <td id=\"T_0d460_row21_col1\" class=\"data row21 col1\" >d429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x247269bddb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d8245_row10_col0, #T_d8245_row10_col1, #T_d8245_row10_col2, #T_d8245_row10_col3, #T_d8245_row10_col4, #T_d8245_row10_col5, #T_d8245_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d8245\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d8245_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_d8245_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_d8245_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_d8245_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_d8245_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_d8245_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_d8245_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d8245_row0_col0\" class=\"data row0 col0\" >0.8275</td>\n",
       "      <td id=\"T_d8245_row0_col1\" class=\"data row0 col1\" >0.9429</td>\n",
       "      <td id=\"T_d8245_row0_col2\" class=\"data row0 col2\" >0.8275</td>\n",
       "      <td id=\"T_d8245_row0_col3\" class=\"data row0 col3\" >0.8267</td>\n",
       "      <td id=\"T_d8245_row0_col4\" class=\"data row0 col4\" >0.8263</td>\n",
       "      <td id=\"T_d8245_row0_col5\" class=\"data row0 col5\" >0.7227</td>\n",
       "      <td id=\"T_d8245_row0_col6\" class=\"data row0 col6\" >0.7237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d8245_row1_col0\" class=\"data row1 col0\" >0.8359</td>\n",
       "      <td id=\"T_d8245_row1_col1\" class=\"data row1 col1\" >0.9463</td>\n",
       "      <td id=\"T_d8245_row1_col2\" class=\"data row1 col2\" >0.8359</td>\n",
       "      <td id=\"T_d8245_row1_col3\" class=\"data row1 col3\" >0.8361</td>\n",
       "      <td id=\"T_d8245_row1_col4\" class=\"data row1 col4\" >0.8351</td>\n",
       "      <td id=\"T_d8245_row1_col5\" class=\"data row1 col5\" >0.7365</td>\n",
       "      <td id=\"T_d8245_row1_col6\" class=\"data row1 col6\" >0.7375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d8245_row2_col0\" class=\"data row2 col0\" >0.8297</td>\n",
       "      <td id=\"T_d8245_row2_col1\" class=\"data row2 col1\" >0.9451</td>\n",
       "      <td id=\"T_d8245_row2_col2\" class=\"data row2 col2\" >0.8297</td>\n",
       "      <td id=\"T_d8245_row2_col3\" class=\"data row2 col3\" >0.8284</td>\n",
       "      <td id=\"T_d8245_row2_col4\" class=\"data row2 col4\" >0.8279</td>\n",
       "      <td id=\"T_d8245_row2_col5\" class=\"data row2 col5\" >0.7257</td>\n",
       "      <td id=\"T_d8245_row2_col6\" class=\"data row2 col6\" >0.7270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d8245_row3_col0\" class=\"data row3 col0\" >0.8265</td>\n",
       "      <td id=\"T_d8245_row3_col1\" class=\"data row3 col1\" >0.9425</td>\n",
       "      <td id=\"T_d8245_row3_col2\" class=\"data row3 col2\" >0.8265</td>\n",
       "      <td id=\"T_d8245_row3_col3\" class=\"data row3 col3\" >0.8284</td>\n",
       "      <td id=\"T_d8245_row3_col4\" class=\"data row3 col4\" >0.8260</td>\n",
       "      <td id=\"T_d8245_row3_col5\" class=\"data row3 col5\" >0.7214</td>\n",
       "      <td id=\"T_d8245_row3_col6\" class=\"data row3 col6\" >0.7229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d8245_row4_col0\" class=\"data row4 col0\" >0.8370</td>\n",
       "      <td id=\"T_d8245_row4_col1\" class=\"data row4 col1\" >0.9449</td>\n",
       "      <td id=\"T_d8245_row4_col2\" class=\"data row4 col2\" >0.8370</td>\n",
       "      <td id=\"T_d8245_row4_col3\" class=\"data row4 col3\" >0.8364</td>\n",
       "      <td id=\"T_d8245_row4_col4\" class=\"data row4 col4\" >0.8353</td>\n",
       "      <td id=\"T_d8245_row4_col5\" class=\"data row4 col5\" >0.7374</td>\n",
       "      <td id=\"T_d8245_row4_col6\" class=\"data row4 col6\" >0.7390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d8245_row5_col0\" class=\"data row5 col0\" >0.8295</td>\n",
       "      <td id=\"T_d8245_row5_col1\" class=\"data row5 col1\" >0.9398</td>\n",
       "      <td id=\"T_d8245_row5_col2\" class=\"data row5 col2\" >0.8295</td>\n",
       "      <td id=\"T_d8245_row5_col3\" class=\"data row5 col3\" >0.8276</td>\n",
       "      <td id=\"T_d8245_row5_col4\" class=\"data row5 col4\" >0.8269</td>\n",
       "      <td id=\"T_d8245_row5_col5\" class=\"data row5 col5\" >0.7246</td>\n",
       "      <td id=\"T_d8245_row5_col6\" class=\"data row5 col6\" >0.7265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d8245_row6_col0\" class=\"data row6 col0\" >0.8325</td>\n",
       "      <td id=\"T_d8245_row6_col1\" class=\"data row6 col1\" >0.9449</td>\n",
       "      <td id=\"T_d8245_row6_col2\" class=\"data row6 col2\" >0.8325</td>\n",
       "      <td id=\"T_d8245_row6_col3\" class=\"data row6 col3\" >0.8312</td>\n",
       "      <td id=\"T_d8245_row6_col4\" class=\"data row6 col4\" >0.8306</td>\n",
       "      <td id=\"T_d8245_row6_col5\" class=\"data row6 col5\" >0.7302</td>\n",
       "      <td id=\"T_d8245_row6_col6\" class=\"data row6 col6\" >0.7315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d8245_row7_col0\" class=\"data row7 col0\" >0.8228</td>\n",
       "      <td id=\"T_d8245_row7_col1\" class=\"data row7 col1\" >0.9415</td>\n",
       "      <td id=\"T_d8245_row7_col2\" class=\"data row7 col2\" >0.8228</td>\n",
       "      <td id=\"T_d8245_row7_col3\" class=\"data row7 col3\" >0.8232</td>\n",
       "      <td id=\"T_d8245_row7_col4\" class=\"data row7 col4\" >0.8223</td>\n",
       "      <td id=\"T_d8245_row7_col5\" class=\"data row7 col5\" >0.7160</td>\n",
       "      <td id=\"T_d8245_row7_col6\" class=\"data row7 col6\" >0.7168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d8245_row8_col0\" class=\"data row8 col0\" >0.8290</td>\n",
       "      <td id=\"T_d8245_row8_col1\" class=\"data row8 col1\" >0.9447</td>\n",
       "      <td id=\"T_d8245_row8_col2\" class=\"data row8 col2\" >0.8290</td>\n",
       "      <td id=\"T_d8245_row8_col3\" class=\"data row8 col3\" >0.8305</td>\n",
       "      <td id=\"T_d8245_row8_col4\" class=\"data row8 col4\" >0.8283</td>\n",
       "      <td id=\"T_d8245_row8_col5\" class=\"data row8 col5\" >0.7254</td>\n",
       "      <td id=\"T_d8245_row8_col6\" class=\"data row8 col6\" >0.7268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d8245_row9_col0\" class=\"data row9 col0\" >0.8368</td>\n",
       "      <td id=\"T_d8245_row9_col1\" class=\"data row9 col1\" >0.9455</td>\n",
       "      <td id=\"T_d8245_row9_col2\" class=\"data row9 col2\" >0.8368</td>\n",
       "      <td id=\"T_d8245_row9_col3\" class=\"data row9 col3\" >0.8373</td>\n",
       "      <td id=\"T_d8245_row9_col4\" class=\"data row9 col4\" >0.8358</td>\n",
       "      <td id=\"T_d8245_row9_col5\" class=\"data row9 col5\" >0.7376</td>\n",
       "      <td id=\"T_d8245_row9_col6\" class=\"data row9 col6\" >0.7390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_d8245_row10_col0\" class=\"data row10 col0\" >0.8307</td>\n",
       "      <td id=\"T_d8245_row10_col1\" class=\"data row10 col1\" >0.9438</td>\n",
       "      <td id=\"T_d8245_row10_col2\" class=\"data row10 col2\" >0.8307</td>\n",
       "      <td id=\"T_d8245_row10_col3\" class=\"data row10 col3\" >0.8306</td>\n",
       "      <td id=\"T_d8245_row10_col4\" class=\"data row10 col4\" >0.8295</td>\n",
       "      <td id=\"T_d8245_row10_col5\" class=\"data row10 col5\" >0.7277</td>\n",
       "      <td id=\"T_d8245_row10_col6\" class=\"data row10 col6\" >0.7291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d8245_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_d8245_row11_col0\" class=\"data row11 col0\" >0.0045</td>\n",
       "      <td id=\"T_d8245_row11_col1\" class=\"data row11 col1\" >0.0019</td>\n",
       "      <td id=\"T_d8245_row11_col2\" class=\"data row11 col2\" >0.0045</td>\n",
       "      <td id=\"T_d8245_row11_col3\" class=\"data row11 col3\" >0.0044</td>\n",
       "      <td id=\"T_d8245_row11_col4\" class=\"data row11 col4\" >0.0044</td>\n",
       "      <td id=\"T_d8245_row11_col5\" class=\"data row11 col5\" >0.0070</td>\n",
       "      <td id=\"T_d8245_row11_col6\" class=\"data row11 col6\" >0.0071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2481160cd00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c59cf_row5_col0, #T_c59cf_row5_col1, #T_c59cf_row5_col2, #T_c59cf_row5_col3, #T_c59cf_row5_col4, #T_c59cf_row5_col5, #T_c59cf_row5_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c59cf\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c59cf_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_c59cf_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_c59cf_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_c59cf_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_c59cf_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_c59cf_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_c59cf_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c59cf_row0_col0\" class=\"data row0 col0\" >0.8288</td>\n",
       "      <td id=\"T_c59cf_row0_col1\" class=\"data row0 col1\" >0.9437</td>\n",
       "      <td id=\"T_c59cf_row0_col2\" class=\"data row0 col2\" >0.8288</td>\n",
       "      <td id=\"T_c59cf_row0_col3\" class=\"data row0 col3\" >0.8276</td>\n",
       "      <td id=\"T_c59cf_row0_col4\" class=\"data row0 col4\" >0.8274</td>\n",
       "      <td id=\"T_c59cf_row0_col5\" class=\"data row0 col5\" >0.7248</td>\n",
       "      <td id=\"T_c59cf_row0_col6\" class=\"data row0 col6\" >0.7257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c59cf_row1_col0\" class=\"data row1 col0\" >0.8298</td>\n",
       "      <td id=\"T_c59cf_row1_col1\" class=\"data row1 col1\" >0.9431</td>\n",
       "      <td id=\"T_c59cf_row1_col2\" class=\"data row1 col2\" >0.8298</td>\n",
       "      <td id=\"T_c59cf_row1_col3\" class=\"data row1 col3\" >0.8284</td>\n",
       "      <td id=\"T_c59cf_row1_col4\" class=\"data row1 col4\" >0.8280</td>\n",
       "      <td id=\"T_c59cf_row1_col5\" class=\"data row1 col5\" >0.7259</td>\n",
       "      <td id=\"T_c59cf_row1_col6\" class=\"data row1 col6\" >0.7272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c59cf_row2_col0\" class=\"data row2 col0\" >0.8333</td>\n",
       "      <td id=\"T_c59cf_row2_col1\" class=\"data row2 col1\" >0.9422</td>\n",
       "      <td id=\"T_c59cf_row2_col2\" class=\"data row2 col2\" >0.8333</td>\n",
       "      <td id=\"T_c59cf_row2_col3\" class=\"data row2 col3\" >0.8314</td>\n",
       "      <td id=\"T_c59cf_row2_col4\" class=\"data row2 col4\" >0.8309</td>\n",
       "      <td id=\"T_c59cf_row2_col5\" class=\"data row2 col5\" >0.7308</td>\n",
       "      <td id=\"T_c59cf_row2_col6\" class=\"data row2 col6\" >0.7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c59cf_row3_col0\" class=\"data row3 col0\" >0.8278</td>\n",
       "      <td id=\"T_c59cf_row3_col1\" class=\"data row3 col1\" >0.9429</td>\n",
       "      <td id=\"T_c59cf_row3_col2\" class=\"data row3 col2\" >0.8278</td>\n",
       "      <td id=\"T_c59cf_row3_col3\" class=\"data row3 col3\" >0.8256</td>\n",
       "      <td id=\"T_c59cf_row3_col4\" class=\"data row3 col4\" >0.8258</td>\n",
       "      <td id=\"T_c59cf_row3_col5\" class=\"data row3 col5\" >0.7228</td>\n",
       "      <td id=\"T_c59cf_row3_col6\" class=\"data row3 col6\" >0.7238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c59cf_row4_col0\" class=\"data row4 col0\" >0.8333</td>\n",
       "      <td id=\"T_c59cf_row4_col1\" class=\"data row4 col1\" >0.9452</td>\n",
       "      <td id=\"T_c59cf_row4_col2\" class=\"data row4 col2\" >0.8333</td>\n",
       "      <td id=\"T_c59cf_row4_col3\" class=\"data row4 col3\" >0.8333</td>\n",
       "      <td id=\"T_c59cf_row4_col4\" class=\"data row4 col4\" >0.8321</td>\n",
       "      <td id=\"T_c59cf_row4_col5\" class=\"data row4 col5\" >0.7319</td>\n",
       "      <td id=\"T_c59cf_row4_col6\" class=\"data row4 col6\" >0.7332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row5\" class=\"row_heading level0 row5\" >Mean</th>\n",
       "      <td id=\"T_c59cf_row5_col0\" class=\"data row5 col0\" >0.8306</td>\n",
       "      <td id=\"T_c59cf_row5_col1\" class=\"data row5 col1\" >0.9434</td>\n",
       "      <td id=\"T_c59cf_row5_col2\" class=\"data row5 col2\" >0.8306</td>\n",
       "      <td id=\"T_c59cf_row5_col3\" class=\"data row5 col3\" >0.8293</td>\n",
       "      <td id=\"T_c59cf_row5_col4\" class=\"data row5 col4\" >0.8288</td>\n",
       "      <td id=\"T_c59cf_row5_col5\" class=\"data row5 col5\" >0.7272</td>\n",
       "      <td id=\"T_c59cf_row5_col6\" class=\"data row5 col6\" >0.7285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c59cf_level0_row6\" class=\"row_heading level0 row6\" >Std</th>\n",
       "      <td id=\"T_c59cf_row6_col0\" class=\"data row6 col0\" >0.0023</td>\n",
       "      <td id=\"T_c59cf_row6_col1\" class=\"data row6 col1\" >0.0010</td>\n",
       "      <td id=\"T_c59cf_row6_col2\" class=\"data row6 col2\" >0.0023</td>\n",
       "      <td id=\"T_c59cf_row6_col3\" class=\"data row6 col3\" >0.0027</td>\n",
       "      <td id=\"T_c59cf_row6_col4\" class=\"data row6 col4\" >0.0023</td>\n",
       "      <td id=\"T_c59cf_row6_col5\" class=\"data row6 col5\" >0.0035</td>\n",
       "      <td id=\"T_c59cf_row6_col6\" class=\"data row6 col6\" >0.0037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x248dad92ce0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f54c4a1ae5c4720b181a40723c931f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_41fde\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_41fde_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_41fde_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_41fde_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_41fde_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_41fde_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_41fde_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_41fde_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_41fde_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_41fde_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_41fde_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_41fde_row0_col1\" class=\"data row0 col1\" >0.8326</td>\n",
       "      <td id=\"T_41fde_row0_col2\" class=\"data row0 col2\" >0.9441</td>\n",
       "      <td id=\"T_41fde_row0_col3\" class=\"data row0 col3\" >0.8326</td>\n",
       "      <td id=\"T_41fde_row0_col4\" class=\"data row0 col4\" >0.8317</td>\n",
       "      <td id=\"T_41fde_row0_col5\" class=\"data row0 col5\" >0.8309</td>\n",
       "      <td id=\"T_41fde_row0_col6\" class=\"data row0 col6\" >0.7303</td>\n",
       "      <td id=\"T_41fde_row0_col7\" class=\"data row0 col7\" >0.7318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24835eb9870>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m modelo \u001b[38;5;241m=\u001b[39m ML_FLOW_PARCIAL(met\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mod\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m salida \u001b[38;5;241m=\u001b[39m \u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mML_FLOW\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 607\u001b[0m, in \u001b[0;36mML_FLOW_PARCIAL.ML_FLOW\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    604\u001b[0m final_dt, u1, u2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_model(exp_clf101)\n\u001b[0;32m    606\u001b[0m \u001b[38;5;66;03m# Paso 5: Predicción\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_dt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[47], line 548\u001b[0m, in \u001b[0;36mML_FLOW_PARCIAL.predict\u001b[1;34m(self, final_dt)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# Interacción cuantitativa vs categórica\u001b[39;00m\n\u001b[0;32m    547\u001b[0m D5 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mba_pru\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 548\u001b[0m result4 \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(.+?)_(.+?)_1\u001b[39m\u001b[38;5;124m'\u001b[39m, item)\u001b[38;5;241m.\u001b[39mgroups() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuactxy]\n\u001b[0;32m    549\u001b[0m contador \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m result4:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "modelo = ML_FLOW_PARCIAL(met=True, mod=1)\n",
    "salida = modelo.ML_FLOW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b170d086-d1dd-4fe4-8f0d-5773f97ed738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': False, 'message': \"name 'cuantitativas' is not defined\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e7609-734f-423e-a51a-36c108b6f6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
